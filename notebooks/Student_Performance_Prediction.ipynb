{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd515b7",
   "metadata": {},
   "source": [
    "# Student Performance Prediction - EDA & Model Training\n",
    "\n",
    "This notebook explores student performance data and trains predictive models to identify at-risk students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278923b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.preprocess import preprocess_student_data\n",
    "from src.train import train_and_evaluate\n",
    "\n",
    "# Configure visualization\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Load raw data\n",
    "df_raw = pd.read_csv('../data/student_performance_updated_1000.csv')\n",
    "print(f\"Dataset shape: {df_raw.shape}\")\n",
    "print(f\"\\nFirst few records:\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2aa717",
   "metadata": {},
   "source": [
    "## Data Exploration & Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3839dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Data Info:\")\n",
    "print(f\"Columns: {df_raw.columns.tolist()}\")\n",
    "print(f\"\\nData types:\\n{df_raw.dtypes}\")\n",
    "print(f\"\\nMissing values:\\n{df_raw.isnull().sum()}\")\n",
    "print(f\"\\nBasic statistics:\")\n",
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18136fe",
   "metadata": {},
   "source": [
    "## Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target variable distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Final Grade distribution\n",
    "axes[0, 0].hist(df_raw['FinalGrade'], bins=30, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].axvline(70, color='red', linestyle='--', label='Intervention Threshold')\n",
    "axes[0, 0].set_xlabel('Final Grade')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].set_title('Distribution of Final Grades')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Attendance Rate distribution\n",
    "axes[0, 1].hist(df_raw['AttendanceRate'], bins=30, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Attendance Rate (%)')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Distribution of Attendance Rates')\n",
    "\n",
    "# Study Hours distribution\n",
    "axes[1, 0].hist(df_raw['StudyHoursPerWeek'], bins=30, color='lightcoral', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Study Hours Per Week')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Distribution of Study Hours')\n",
    "\n",
    "# Previous Grade distribution\n",
    "axes[1, 1].hist(df_raw['PreviousGrade'], bins=30, color='lightyellow', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Previous Grade')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_title('Distribution of Previous Grades')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nGrades below intervention threshold (70): {(df_raw['FinalGrade'] < 70).sum()} students\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0303398",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f68583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with FinalGrade\n",
    "numeric_cols = df_raw.select_dtypes(include=['int64', 'float64']).columns\n",
    "correlations = df_raw[numeric_cols].corr()['FinalGrade'].sort_values(ascending=False)\n",
    "\n",
    "print(\"Correlation with Final Grade:\")\n",
    "print(correlations)\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_raw[numeric_cols].corr(), annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f98605",
   "metadata": {},
   "source": [
    "## Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e74d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gender vs Final Grade\n",
    "df_raw.groupby('Gender')['FinalGrade'].mean().plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_xlabel('Gender')\n",
    "axes[0].set_ylabel('Average Final Grade')\n",
    "axes[0].set_title('Average Final Grade by Gender')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "\n",
    "# Parental Support vs Final Grade\n",
    "df_raw.groupby('ParentalSupport')['FinalGrade'].mean().sort_values(ascending=False).plot(\n",
    "    kind='bar', ax=axes[1], color='lightgreen')\n",
    "axes[1].set_xlabel('Parental Support')\n",
    "axes[1].set_ylabel('Average Final Grade')\n",
    "axes[1].set_title('Average Final Grade by Parental Support Level')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nParental Support Distribution:\")\n",
    "print(df_raw['ParentalSupport'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f61c583",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "X, y, preprocessor = preprocess_student_data('../data/student_performance_updated_1000.csv')\n",
    "\n",
    "print(f\"\\nProcessed data shape:\")\n",
    "print(f\"Features X: {X.shape}\")\n",
    "print(f\"Target y: {y.shape}\")\n",
    "print(f\"\\nFeature names: {X.columns.tolist()}\")\n",
    "print(f\"\\nFinal Grade statistics (target):\")\n",
    "print(f\"Mean: {y.mean():.2f}\")\n",
    "print(f\"Std: {y.std():.2f}\")\n",
    "print(f\"Min: {y.min():.2f}\")\n",
    "print(f\"Max: {y.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df81e4",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "results, X_train, X_test, y_train, y_test = train_and_evaluate(\n",
    "    X, y, \n",
    "    model_types=['linear', 'tree', 'random_forest']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385063a",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importance across models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, (model_type, result) in enumerate(results.items()):\n",
    "    model = result['model']\n",
    "    if model.feature_importance is not None:\n",
    "        top_features = model.feature_importance.head(10)\n",
    "        axes[idx].barh(range(len(top_features)), top_features['importance'].values)\n",
    "        axes[idx].set_yticks(range(len(top_features)))\n",
    "        axes[idx].set_yticklabels(top_features['feature'].values)\n",
    "        axes[idx].set_xlabel('Importance Score')\n",
    "        axes[idx].set_title(f'{model_type.upper()} - Top 10 Features')\n",
    "        axes[idx].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9555dba",
   "metadata": {},
   "source": [
    "## At-Risk Student Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc737c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze at-risk students for best performing model (Random Forest)\n",
    "best_model_type = 'random_forest'\n",
    "at_risk_df = results[best_model_type]['at_risk']\n",
    "\n",
    "print(f\"\\nAt-Risk Student Details (Top 10):\")\n",
    "print(at_risk_df[at_risk_df['at_risk']].head(10))\n",
    "\n",
    "# Visualize prediction accuracy\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[0].scatter(at_risk_df['actual_grade'], at_risk_df['predicted_grade'], alpha=0.6)\n",
    "axes[0].plot([at_risk_df['actual_grade'].min(), at_risk_df['actual_grade'].max()],\n",
    "            [at_risk_df['actual_grade'].min(), at_risk_df['actual_grade'].max()],\n",
    "            'r--', label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Grade')\n",
    "axes[0].set_ylabel('Predicted Grade')\n",
    "axes[0].set_title('Actual vs Predicted Grades')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction Error Distribution\n",
    "axes[1].hist(at_risk_df['difference'], bins=30, color='lightblue', edgecolor='black')\n",
    "axes[1].axvline(0, color='red', linestyle='--', label='Zero Error')\n",
    "axes[1].set_xlabel('Prediction Error (Actual - Predicted)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Distribution of Prediction Errors')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787de0fe",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best performing model\n",
    "best_model = results['random_forest']['model']\n",
    "model_path = best_model.save_model('../models/student_performance_model.pkl')\n",
    "print(f\"\\nModel saved successfully!\")\n",
    "print(f\"Path: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
